{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    RobertaModel,\n",
    "    VivitModel,\n",
    "    Wav2Vec2Processor,\n",
    "    RobertaTokenizer,\n",
    "    VivitImageProcessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 7.99969482421875 GB\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(0)\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "    free_memory = total_memory - max(allocated_memory, reserved_memory)\n",
    "    print(f\"Available GPU Memory: {free_memory / (1024 ** 3)} GB\")\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tvtla\\OneDrive\\Työpöytä\\repo\\MultimodalEmotionRecognition\\demo.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m outputs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mrandn(\u001b[39m5\u001b[39m, \u001b[39m2\u001b[39m)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(outputs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "outputs = [torch.randn(5, 2)]\n",
    "outputs = torch.stack(outputs)\n",
    "print(outputs.shape)\n",
    "outputs = torch.mean(outputs, dim=0)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 97, 768])\n",
      "Available GPU Memory: 7.43914794921875 GB\n",
      "CPU times: total: 1.59 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load audio file\n",
    "waveform1, sample_rate = torchaudio.load('E:/IEMOCAP_full_release/Session1/audio/Ses01F_impro01/Ses01F_impro01_F000.wav')\n",
    "waveform2, sample_rate = torchaudio.load('E:/IEMOCAP_full_release/Session1/audio/Ses01F_impro01/Ses01F_impro01_F001.wav')\n",
    "\n",
    "waveform1 = waveform1.numpy().squeeze()\n",
    "waveform2 = waveform2.numpy().squeeze()\n",
    "\n",
    "# Load models\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2.gradient_checkpointing_enable()\n",
    "\n",
    "# Evaluate model\n",
    "wav2vec2.eval()\n",
    "\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = audio_processor([waveform1, waveform2], return_tensors='pt', sampling_rate=16000, padding=True)\n",
    "    inputs.to(device)\n",
    "    wav2vec2.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = wav2vec2(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states[1:]\n",
    "    hidden_states = torch.stack(hidden_states, axis=0)\n",
    "\n",
    "    print(hidden_states.shape)\n",
    "\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, wav2vec2\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>Replace me by any text you'd like.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = \"Replace me by any text you'd like.\"\n",
    "\n",
    "\n",
    "text_tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
    "text_inputs = text_tokenizer(text2, return_tensors='pt', padding='max_length', max_length=128)\n",
    "print(text_inputs['input_ids'].shape)\n",
    "text_tokenizer.decode(text_inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "torch.Size([2, 16, 768])\n",
      "torch.Size([12, 2, 16, 768])\n",
      "torch.Size([12, 2, 768])\n",
      "Available GPU Memory: 7.48016357421875 GB\n",
      "CPU times: total: 1.72 s\n",
      "Wall time: 1.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load text file\n",
    "text1 = \"Replace me by any text you'd like.\"\n",
    "text2 = \"Replace me by any text you'd like. asdasd\"\n",
    "\n",
    "\n",
    "# Load models\n",
    "text_inputs = text_tokenizer(text_inputs, return_tensors='pt', truncation=True, do_lower_case=True, padding=True, max_length=128)\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Evaluate model\n",
    "roberta.eval()\n",
    "\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = tokenizer([text1, text2], return_tensors='pt', padding=True)\n",
    "    inputs.to(device)\n",
    "    roberta.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = roberta(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    print(outputs[0][:, 0].shape)\n",
    "    print(hidden_states[0].shape)\n",
    "    print(torch.stack(hidden_states[1:]).shape)\n",
    "    print(torch.stack(hidden_states[1:])[:, :, 0].shape)\n",
    "\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, last_hidden_states, roberta\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.weight', 'vivit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 3137, 768])\n",
      "[2, 3137, 768]\n",
      "Available GPU Memory: 5.47235107421875 GB\n",
      "CPU times: total: 4.17 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load audio file\n",
    "video1, _, _ = torchvision.io.read_video(\"E:/IEMOCAP_full_release/Session1/video/Ses01F_script01_1/Ses01F_script01_1_F039.avi\", output_format=\"THWC\", pts_unit='sec')\n",
    "video2, _, _ = torchvision.io.read_video(\"E:/IEMOCAP_full_release/Session1/video/Ses01F_script01_1/Ses01F_script01_1_F038.avi\", output_format=\"THWC\", pts_unit='sec')\n",
    "\n",
    "# Load models\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "vivit = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "indices1 = np.linspace(0, video1.shape[0] - 1, 32, dtype=int)\n",
    "indices2 = np.linspace(0, video2.shape[0] - 1, 32, dtype=int)\n",
    "\n",
    "video1 = video1[indices1]\n",
    "video1_frames = [video1[i] for i in range(video1.shape[0])]\n",
    "video2 = video2[indices2]\n",
    "video2_frames = [video2[i] for i in range(video2.shape[0])]\n",
    "\n",
    "# Evaluate model\n",
    "vivit.eval()\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = image_processor([video1_frames, video2_frames], return_tensors=\"pt\", padding=True)\n",
    "    inputs.to(device)\n",
    "    vivit.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = vivit(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Print shape\n",
    "print(torch.stack(hidden_states[1:], axis=0).shape)\n",
    "print(list(last_hidden_state.shape))\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, last_hidden_state, vivit\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotionrecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
