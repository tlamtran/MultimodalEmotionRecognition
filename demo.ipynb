{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Model,\n",
    "    RobertaModel,\n",
    "    VivitModel,\n",
    "    Wav2Vec2Processor,\n",
    "    RobertaTokenizer,\n",
    "    VivitImageProcessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Memory: 7.99969482421875 GB\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_memory():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "    allocated_memory = torch.cuda.memory_allocated(0)\n",
    "    reserved_memory = torch.cuda.memory_reserved(0)\n",
    "\n",
    "    free_memory = total_memory - max(allocated_memory, reserved_memory)\n",
    "    print(f\"Available GPU Memory: {free_memory / (1024 ** 3)} GB\")\n",
    "\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0579, 0.0873, 0.0681, 0.0319, 0.0161, 0.0089, 0.0077, 0.4500, 0.1338,\n",
      "        0.0418, 0.0611, 0.0354], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "weights = nn.Parameter(torch.randn(12))\n",
    "weights = F.softmax(weights, dim=0)\n",
    "print(weights)\n",
    "weights = weights.view(12, 1, 1, 1)\n",
    "hidden_states = torch.randn(12, 2, 97, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 97, 768])\n",
      "Available GPU Memory: 7.43914794921875 GB\n",
      "CPU times: total: 1.59 s\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load audio file\n",
    "waveform1, sample_rate = torchaudio.load('E:/IEMOCAP_full_release/Session1/audio/Ses01F_impro01/Ses01F_impro01_F000.wav')\n",
    "waveform2, sample_rate = torchaudio.load('E:/IEMOCAP_full_release/Session1/audio/Ses01F_impro01/Ses01F_impro01_F001.wav')\n",
    "\n",
    "waveform1 = waveform1.numpy().squeeze()\n",
    "waveform2 = waveform2.numpy().squeeze()\n",
    "\n",
    "# Load models\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2.gradient_checkpointing_enable()\n",
    "\n",
    "# Evaluate model\n",
    "wav2vec2.eval()\n",
    "\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = audio_processor([waveform1, waveform2], return_tensors='pt', sampling_rate=16000, padding=True)\n",
    "    inputs.to(device)\n",
    "    wav2vec2.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = wav2vec2(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states[1:]\n",
    "    hidden_states = torch.stack(hidden_states, axis=0)\n",
    "\n",
    "    print(hidden_states.shape)\n",
    "\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, wav2vec2\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n",
      "13\n",
      "[2, 16, 768]\n",
      "Available GPU Memory: 7.47625732421875 GB\n",
      "CPU times: total: 2.23 s\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load text file\n",
    "text1 = \"Replace me by any text you'd like.\"\n",
    "text2 = \"Replace me by any text you'd like. asdasd\"\n",
    "\n",
    "\n",
    "# Load models\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Evaluate model\n",
    "roberta.eval()\n",
    "\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = tokenizer([text1, text2], return_tensors='pt', padding=True)\n",
    "    print(inputs['input_ids'].shape)\n",
    "    inputs.to(device)\n",
    "    roberta.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = roberta(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Print shape\n",
    "print(len(hidden_states))\n",
    "print(list(last_hidden_states.shape))\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, last_hidden_states, roberta\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.weight', 'vivit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 2, 3137, 768])\n",
      "[2, 3137, 768]\n",
      "Available GPU Memory: 5.47235107421875 GB\n",
      "CPU times: total: 4.17 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load audio file\n",
    "video1, _, _ = torchvision.io.read_video(\"E:/IEMOCAP_full_release/Session1/video/Ses01F_script01_1/Ses01F_script01_1_F039.avi\", output_format=\"THWC\", pts_unit='sec')\n",
    "video2, _, _ = torchvision.io.read_video(\"E:/IEMOCAP_full_release/Session1/video/Ses01F_script01_1/Ses01F_script01_1_F038.avi\", output_format=\"THWC\", pts_unit='sec')\n",
    "\n",
    "# Load models\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "vivit = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "indices1 = np.linspace(0, video1.shape[0] - 1, 32, dtype=int)\n",
    "indices2 = np.linspace(0, video2.shape[0] - 1, 32, dtype=int)\n",
    "\n",
    "video1 = video1[indices1]\n",
    "video1_frames = [video1[i] for i in range(video1.shape[0])]\n",
    "video2 = video2[indices2]\n",
    "video2_frames = [video2[i] for i in range(video2.shape[0])]\n",
    "\n",
    "# Evaluate model\n",
    "vivit.eval()\n",
    "# Process inputs\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    inputs = image_processor([video1_frames, video2_frames], return_tensors=\"pt\", padding=True)\n",
    "    inputs.to(device)\n",
    "    vivit.to(device)\n",
    "\n",
    "    # Get outputs\n",
    "    outputs = vivit(**inputs, output_hidden_states=True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "# Print shape\n",
    "print(torch.stack(hidden_states[1:], axis=0).shape)\n",
    "print(list(last_hidden_state.shape))\n",
    "\n",
    "# Free memory\n",
    "get_gpu_memory()\n",
    "\n",
    "del inputs, outputs, hidden_states, last_hidden_state, vivit\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import iemocap\n",
    "\n",
    "dataset = iemocap.IEMOCAP('E:/IEMOCAP_full_release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IEMOCAP' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tvtla\\OneDrive\\Työpöytä\\repo\\MultimodalEmotionRecognition\\demo.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tvtla/OneDrive/Ty%C3%B6p%C3%B6yt%C3%A4/repo/MultimodalEmotionRecognition/demo.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'IEMOCAP' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(\n",
    "    batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_input, text_input, video_input, label = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 10105,    38,    21,  2445,    13,    47,     6,  1573,     4,\n",
      "          1437, 15628,   172,    47,   393,   875,   162,     8,   172,    77,\n",
      "            47,   222,     6,   157,     6,    47,   686,    64,    28, 33406,\n",
      "            47,   216,     4,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(text_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotionrecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
