{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tvtla\\miniconda3\\envs\\pytorch-gpu\\lib\\site-packages\\transformers\\configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 97, 768]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torchaudio\n",
    "\n",
    "waveform, sample_rate = torchaudio.load('E:/IEMOCAP_full_release/Session1/audio/Ses01F_impro01/Ses01F_impro01_F000.wav')\n",
    "\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "wav2vec2.eval()\n",
    "\n",
    "inputs = audio_processor(waveform[0], return_tensors='pt', sampling_rate=16000)\n",
    "\n",
    "inputs.to(device)\n",
    "wav2vec2.to(device)\n",
    "\n",
    "outputs = wav2vec2(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 12, 768]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "roberta.eval()\n",
    "\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "inputs = tokenizer([text, text], return_tensors='pt')\n",
    "\n",
    "inputs.to(device)\n",
    "roberta.to(device)\n",
    "\n",
    "outputs = roberta(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VivitModel were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized: ['vivit.pooler.dense.bias', 'vivit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3137, 768]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VivitImageProcessor, VivitModel\n",
    "from torchvision.io.video import read_video\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "video, _, _ = read_video(\"E:/IEMOCAP_full_release/Session1/video/Ses01F_impro01/Ses01F_impro01_F000.avi\", output_format=\"THWC\", pts_unit='sec')\n",
    "\n",
    "image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "vivit = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n",
    "\n",
    "indices = np.linspace(0, video.shape[0] - 1, 32, dtype=int)\n",
    "inputs = image_processor(list(video[indices]), return_tensors=\"pt\")\n",
    "\n",
    "vivit.eval()\n",
    "\n",
    "inputs.to(device)\n",
    "vivit.to(device)\n",
    "\n",
    "outputs = vivit(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotionrecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
